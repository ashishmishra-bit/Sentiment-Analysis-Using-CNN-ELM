{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b747743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3566d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Activation, Dropout, Flatten\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from tensorflow.keras.optimizers import sgd\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae863aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=(256,256)\n",
    "imageShape = (dim[0],dim[1],3)\n",
    "numClasses = 2\n",
    "batchSize = 10\n",
    "epochs = 1\n",
    "folderWithPics='twitter'\n",
    "dirs=os.listdir('./'+folderWithPics)\n",
    "clsLabels=pd.read_csv('./'+folderWithPics+'/groundTruthLabel.txt',delimiter='\\t')\n",
    "clsLabels.index=clsLabels.index+1\n",
    "subDirPath=[('./'+folderWithPics+'/'+di) for di in dirs if('txt' not in di)]\n",
    "allImagesTrainPath=[(si+'/'+ii) for si in subDirPath[:-1] for ii in os.listdir(si) if('jpg' in ii)]\n",
    "allImagesTestPath=[(si+'/'+ii) for si in [subDirPath[-1]] for ii in os.listdir(si) if('jpg' in ii)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9a003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formImageSet(allImagesFoldrPath,dim,clsLabels):\n",
    "    x_imageSet=np.empty((len(allImagesFoldrPath),dim[0],dim[1],3))\n",
    "    y_Set=np.empty((len(allImagesFoldrPath),1))\n",
    "    for im in range(len(allImagesFoldrPath)):\n",
    "        readImage=imread(allImagesFoldrPath[im])\n",
    "        \n",
    "        imNum=int(allImagesFoldrPath[im].split('/')[-1].split('.')[0])\n",
    "        actualClass=clsLabels.loc[imNum][1]\n",
    "        \n",
    "        if (actualClass=='positive'):\n",
    "            y_Set[im]=1\n",
    "        else:\n",
    "            y_Set[im]=0\n",
    "            \n",
    "        if (len(readImage.shape)>=3):\n",
    "            if readImage.shape[2]>3:\n",
    "                readImage=readImage[:,:,:3]            \n",
    "        else:\n",
    "            print(im,readImage.shape)\n",
    "            readImage=gray2rgb(readImage)            \n",
    "        readImage=resize(readImage,dim)\n",
    "        x_imageSet[im]=readImage\n",
    "    return x_imageSet,y_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba91a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataSet():\n",
    "    xTrainImSet,yTrainSet=formImageSet(allImagesTrainPath,dim,clsLabels)\n",
    "    xTestImSet,yTestSet=formImageSet(allImagesTestPath,dim,clsLabels)\n",
    "    \n",
    "    xTrainImSet= xTrainImSet.astype('float32')\n",
    "    xTestImSet= xTestImSet.astype('float32')\n",
    "    xTrainImSet /= 255.0\n",
    "    xTestImSet /= 255.0\n",
    "\n",
    "    yTrainSet= keras.utils.np_utils.to_categorical(yTrainSet, numClasses)\n",
    "    yTestSet= keras.utils.np_utils.to_categorical(yTestSet, numClasses)\n",
    "    \n",
    "    print('Train Dataset size: ', xTrainImSet.shape[0])\n",
    "    print('Test Dataset size: ', yTestSet.shape[0])\n",
    "    \n",
    "    return (xTrainImSet,yTrainSet), (xTestImSet,yTestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "459f7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAModel():\n",
    "# The sequential model of keras is used\n",
    "    model = Sequential()\n",
    "    \n",
    "# 1st Convolution layer with 16 filters\n",
    "    model.add(Conv2D(16, kernel_size=(11,11), padding='same', \n",
    "\t\t\t\t\t kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='zeros', \n",
    "                     input_shape=imageShape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))       \n",
    "\n",
    "# 2nd Convolution layer with 96 filters\n",
    "    model.add(Conv2D(96, kernel_size=(1,1), padding='same', \n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "              \n",
    "#3rd Convolution layer with 192 filters\n",
    "    model.add(Conv2D(192, kernel_size=(5,5), padding='same', \n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "              \n",
    "#4th Convolution layer with 192 filters\n",
    "    model.add(Conv2D(192, kernel_size=(1,1), padding='same',\n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "    \n",
    "#5th Convolution layer with 192 filters\n",
    "    model.add(Conv2D(192, kernel_size=(3,3), padding='same',\n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#6th Convolution layer with 192 filters\n",
    "    model.add(Conv2D(192, kernel_size=(1,1), padding='same',\n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#7th Convolution layer with 10 filters\n",
    "    model.add(Conv2D(10, kernel_size=(1,1), padding='same',\n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(AveragePooling2D(pool_size=(6,6)))\n",
    "\n",
    "#8th Flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "#9th Dense layer \n",
    "    model.add(Dense(numClasses, kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb003599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data set...\n",
      "245 (277, 500)\n",
      "Train Dataset size:  482\n",
      "Test Dataset size:  121\n"
     ]
    }
   ],
   "source": [
    "print('Prepare data set...')\n",
    "(xTrainImSet,yTrainSet), (xTestImSet,yTestSet) = prepareDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1758aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a model...\n"
     ]
    }
   ],
   "source": [
    "print('Create a model...')\n",
    "model = createAModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b6426df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the optimizer and compile the model\n"
     ]
    }
   ],
   "source": [
    "print('Set the optimizer and compile the model')\n",
    "# optimizer = sgd(0.01, 0.8, 0.0005, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27f5d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model\n",
      "49/49 [==============================] - 82s 2s/step - loss: 0.5660 - accuracy: 0.7697 - val_loss: 0.5617 - val_accuracy: 0.8182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2845332e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train the model')\n",
    "model.fit(xTrainImSet, yTrainSet,batch_size=batchSize,epochs=epochs,validation_data=(xTestImSet, yTestSet),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "334fd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesing the model\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.5836 - accuracy: 0.8182\n",
      "Test accuracy:  0.8181818127632141 Test loss:  0.5836220383644104\n"
     ]
    }
   ],
   "source": [
    "print('Tesing the model')\n",
    "score = model.evaluate(xTestImSet, yTestSet)\n",
    "print('Test accuracy: ', score[1],'Test loss: ', score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249d664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
